#!/usr/bin/env python3
"""
æ™ºèƒ½æœç´¢Agent - Geminiç‰ˆ
ä½¿ç”¨Google Gemini APIä»£æ›¿Claude APIè¿›è¡Œæ™ºèƒ½æœç´¢
"""

import os
import json
import logging
import urllib.parse
import urllib.request
import xml.etree.ElementTree as ET
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from dotenv import load_dotenv
from google import genai

# å¯¼å…¥å·²æœ‰çš„APIæµ‹è¯•ç±»
from test_arxiv_api import ArxivAPITester
from test_wikipedia_api import WikipediaAPITester
from test_google_scholar_api import GoogleScholarAPITester
from test_gemini_api import GeminiAPITester

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DebugInfo:
    """DEBUGä¿¡æ¯æ”¶é›†å™¨"""
    def __init__(self):
        self.logs = []
    
    def add_log(self, stage: str, data: Dict[str, Any]):
        """æ·»åŠ DEBUGæ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "stage": stage,
            "data": data
        }
        self.logs.append(log_entry)
        logger.debug(json.dumps(log_entry, ensure_ascii=False, indent=2))
    
    def get_logs(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ—¥å¿—"""
        return self.logs

class GeminiAPI:
    """Gemini APIè°ƒç”¨å™¨"""
    def __init__(self, api_key: Optional[str] = None):
        # æ£€æŸ¥APIå¯†é’¥
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            logger.warning("æœªæ‰¾åˆ°GOOGLE_API_KEYç¯å¢ƒå˜é‡ï¼ŒGemini APIå°†æ— æ³•ä½¿ç”¨")
            self.client = None
            self.import_error = "APIå¯†é’¥æœªè®¾ç½®"
            return

        # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
        try:
            self.client = genai.Client(api_key=self.api_key)
            self.import_error = None
        except Exception as e:
            logger.error(f"Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            self.import_error = str(e)
    
    def call(self, prompt: str, model_name: str = "gemini-2.0-flash-001") -> str:
        """è°ƒç”¨Gemini API"""
        if not self.client:
            return f"è°ƒç”¨å¤±è´¥: Gemini APIæœªåˆå§‹åŒ– - {self.import_error}"
        
        try:
            # ä½¿ç”¨Geminiç”Ÿæˆå†…å®¹
            response = self.client.models.generate_content(
                model=model_name,
                contents=prompt
            )
            
            # ä»å“åº”ä¸­æå–æ–‡æœ¬
            if response and response.text:
                return response.text
            else:
                return "è°ƒç”¨å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”"
                
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            return f"è°ƒç”¨å¤±è´¥: {str(e)}"

class SearchAPIs:
    """æœç´¢APIé›†åˆ"""
    def __init__(self, debug_info: DebugInfo):
        self.debug_info = debug_info
        self.arxiv = ArxivAPITester()
        self.wikipedia = WikipediaAPITester()
        self.google_scholar = GoogleScholarAPITester()
    
    def search_arxiv(self, query: str) -> Dict[str, Any]:
        """æœç´¢arXivè®ºæ–‡"""
        self.debug_info.add_log("arxiv_search_start", {"query": query})
        try:
            # æ„å»ºæŸ¥è¯¢
            encoded_query = urllib.parse.quote(query)
            url = f'{self.arxiv.base_url}?search_query=all:{encoded_query}&start=0&max_results=5'
            
            with urllib.request.urlopen(url, timeout=20) as response:
                data = response.read().decode('utf-8')
            
            root = ET.fromstring(data)
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('.//atom:entry', namespaces)
            results = []
            
            for entry in entries:
                title = entry.find('./atom:title', namespaces).text.strip()
                summary = entry.find('./atom:summary', namespaces).text.strip()
                authors = [author.find('./atom:name', namespaces).text 
                          for author in entry.findall('./atom:author', namespaces)]
                published = entry.find('./atom:published', namespaces).text
                
                # è·å–arXiv IDå’Œé“¾æ¥
                id_elem = entry.find('./atom:id', namespaces)
                arxiv_id = id_elem.text.split('/')[-1] if id_elem is not None else ""
                
                results.append({
                    "title": title,
                    "summary": summary[:300] + "..." if len(summary) > 300 else summary,
                    "authors": authors[:3],
                    "published": published,
                    "arxiv_id": arxiv_id,
                    "link": f"https://arxiv.org/abs/{arxiv_id}"
                })
            
            self.debug_info.add_log("arxiv_search_complete", {
                "query": query,
                "results_count": len(results)
            })
            
            return {"status": "success", "results": results}
            
        except Exception as e:
            error_msg = f"arXivæœç´¢å¤±è´¥: {str(e)}"
            self.debug_info.add_log("arxiv_search_error", {"error": str(e)})
            return {"status": "error", "error": error_msg}
    
    def search_wikipedia(self, query: str) -> Dict[str, Any]:
        """æœç´¢Wikipedia"""
        self.debug_info.add_log("wikipedia_search_start", {"query": query})
        try:
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': query,
                'srlimit': 5,
                'srprop': 'snippet|titlesnippet|size|wordcount|timestamp'
            }
            
            response = self.wikipedia.session.get(
                self.wikipedia.base_url, 
                params=params, 
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            if 'query' in data and 'search' in data['query']:
                for item in data['query']['search']:
                    results.append({
                        "title": item.get('title', ''),
                        "snippet": item.get('snippet', '').replace('<span class="searchmatch">', '').replace('</span>', ''),
                        "size": item.get('size', 0),
                        "wordcount": item.get('wordcount', 0),
                        "timestamp": item.get('timestamp', ''),
                        "link": f"https://zh.wikipedia.org/wiki/{urllib.parse.quote(item.get('title', ''))}"
                    })
            
            self.debug_info.add_log("wikipedia_search_complete", {
                "query": query,
                "results_count": len(results)
            })
            
            return {"status": "success", "results": results}
            
        except Exception as e:
            error_msg = f"Wikipediaæœç´¢å¤±è´¥: {str(e)}"
            self.debug_info.add_log("wikipedia_search_error", {"error": str(e)})
            return {"status": "error", "error": error_msg}
    
    def search_google_scholar(self, query: str) -> Dict[str, Any]:
        """æœç´¢Google Scholar"""
        self.debug_info.add_log("google_scholar_search_start", {"query": query})
        try:
            if not hasattr(self.google_scholar, 'api_key') or not self.google_scholar.api_key:
                return {"status": "error", "error": "Google Scholar APIå¯†é’¥æœªè®¾ç½®"}
            
            params = self.google_scholar.params.copy()
            params["q"] = query
            
            response = requests.get(
                self.google_scholar.api_base,
                params=params,
                timeout=20
            )
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for item in data.get("organic_results", [])[:5]:
                results.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "link": item.get("link", ""),
                    "publication_info": item.get("publication_info", {}).get("summary", ""),
                    "cited_by": item.get("inline_links", {}).get("cited_by", {}).get("total", 0),
                    "authors": [author.get("name", "") for author in item.get("publication_info", {}).get("authors", [])][:3]
                })
            
            self.debug_info.add_log("google_scholar_search_complete", {
                "query": query,
                "results_count": len(results)
            })
            
            return {"status": "success", "results": results}
            
        except Exception as e:
            error_msg = f"Google Scholaræœç´¢å¤±è´¥: {str(e)}"
            self.debug_info.add_log("google_scholar_search_error", {"error": str(e)})
            return {"status": "error", "error": error_msg}

class IntelligentSearchAgent:
    """æ™ºèƒ½æœç´¢Agentä¸»ç±»"""
    
    def __init__(self, google_api_key: Optional[str] = None):
        self.debug_info = DebugInfo()
        self.gemini_api = GeminiAPI(api_key=google_api_key)
        self.search_apis = SearchAPIs(self.debug_info)
    
    def analyze_query(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œå†³å®šä½¿ç”¨å“ªäº›æœç´¢æº"""
        self.debug_info.add_log("query_analysis_start", {"query": query})
        
        # ä½¿ç”¨Geminiåˆ†æé—®é¢˜
        analysis_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œåˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªäº›æœç´¢æºæ¥è·å–ä¿¡æ¯ï¼š
        
        é—®é¢˜ï¼š{query}
        
        å¯ç”¨çš„æœç´¢æºï¼š
        1. arxiv - å­¦æœ¯è®ºæ–‡ã€é¢„å°æœ¬ã€ç§‘ç ”æˆæœ
        2. wikipedia - é€šç”¨çŸ¥è¯†ã€æ¦‚å¿µè§£é‡Šã€å†å²äº‹ä»¶  
        3. google_scholar - å­¦æœ¯æ–‡çŒ®ã€æœŸåˆŠè®ºæ–‡ã€å¼•ç”¨ç»Ÿè®¡
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
        {{
            "query_type": "å­¦æœ¯/é€šç”¨/æ··åˆ",
            "recommended_sources": ["source1", "source2"],
            "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "reasoning": "é€‰æ‹©ç†ç”±"
        }}

        åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚
        """
        
        try:
            response = self.gemini_api.call(analysis_prompt)
            # å°è¯•è§£æJSON
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                analysis_result = json.loads(json_match.group())
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
                analysis_result = {
                    "query_type": "æ··åˆ",
                    "recommended_sources": ["arxiv", "wikipedia"],
                    "search_keywords": [query],
                    "reasoning": "é»˜è®¤æœç´¢ç­–ç•¥"
                }
            
            self.debug_info.add_log("query_analysis_complete", analysis_result)
            return analysis_result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢åˆ†æå¤±è´¥: {e}")
            self.debug_info.add_log("query_analysis_error", {"error": str(e)})
            return {
                "query_type": "æ··åˆ",
                "recommended_sources": ["arxiv", "wikipedia"],
                "search_keywords": [query],
                "reasoning": "åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥"
            }
    
    def parallel_search(self, query: str, sources: List[str]) -> Dict[str, Any]:
        """å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®æº"""
        self.debug_info.add_log("parallel_search_start", {
            "query": query,
            "sources": sources
        })
        
        search_results = {}
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæœç´¢
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_source = {}
            
            for source in sources:
                if source == "arxiv":
                    future = executor.submit(self.search_apis.search_arxiv, query)
                    future_to_source[future] = "arxiv"
                elif source == "wikipedia":
                    future = executor.submit(self.search_apis.search_wikipedia, query)
                    future_to_source[future] = "wikipedia"
                elif source == "google_scholar":
                    future = executor.submit(self.search_apis.search_google_scholar, query)
                    future_to_source[future] = "google_scholar"
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    result = future.result()
                    search_results[source] = result
                except Exception as e:
                    search_results[source] = {"status": "error", "error": str(e)}
        
        self.debug_info.add_log("parallel_search_complete", {
            "sources_searched": list(search_results.keys()),
            "success_count": sum(1 for r in search_results.values() if r.get("status") == "success")
        })
        
        return search_results
    
    def summarize_results(self, query: str, search_results: Dict[str, Any]) -> str:
        """ä½¿ç”¨Geminiæ±‡æ€»æœç´¢ç»“æœ"""
        self.debug_info.add_log("summarization_start", {
            "query": query,
            "sources_count": len(search_results)
        })
        
        # å‡†å¤‡æœç´¢ç»“æœæ‘˜è¦
        results_summary = {}
        for source, data in search_results.items():
            if data.get("status") == "success":
                results_summary[source] = data.get("results", [])
        
        # æ„å»ºæ±‡æ€»æç¤º
        summary_prompt = f"""
        åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œè¯·ä¸ºç”¨æˆ·é—®é¢˜æä¾›ä¸€ä¸ªå…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚

        ç”¨æˆ·é—®é¢˜ï¼š{query}

        æœç´¢ç»“æœï¼š
        {json.dumps(results_summary, ensure_ascii=False, indent=2)}

        è¯·æä¾›ï¼š
        1. ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆå¼€é—¨è§å±±ï¼‰
        2. æ•´åˆå„ä¸ªæ¥æºçš„ä¿¡æ¯ï¼ˆå‡†ç¡®å¼•ç”¨ï¼‰
        3. å¦‚æœæœ‰å­¦æœ¯è®ºæ–‡ï¼Œè¯·ç‰¹åˆ«æåŠæ ‡é¢˜å’Œä½œè€…
        4. å¦‚æœæœ‰Wikipediaå†…å®¹ï¼Œè¯´æ˜åŸºç¡€æ¦‚å¿µ
        5. æ ‡æ³¨æ¯ä¸ªé‡è¦ä¿¡æ¯çš„æ¥æº

        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯è¯»æ€§ã€‚
        """
        
        try:
            # å¯¹è¾ƒé•¿çš„å†…å®¹ä½¿ç”¨Proæ¨¡å‹
            model = "gemini-2.0-pro-001" if len(json.dumps(results_summary)) > 10000 else "gemini-2.0-flash-001"
            summary = self.gemini_api.call(summary_prompt, model)
            self.debug_info.add_log("summarization_complete", {
                "summary_length": len(summary),
                "model_used": model
            })
            return summary
        except Exception as e:
            error_msg = f"æ±‡æ€»å¤±è´¥: {str(e)}"
            self.debug_info.add_log("summarization_error", {"error": str(e)})
            return error_msg
    
    def search(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½æœç´¢çš„ä¸»æ–¹æ³•"""
        self.debug_info.add_log("search_start", {"query": query})
        
        try:
            # 1. åˆ†ææŸ¥è¯¢
            analysis = self.analyze_query(query)
            
            # 2. å¹¶è¡Œæœç´¢
            search_results = self.parallel_search(
                query, 
                analysis.get("recommended_sources", ["arxiv", "wikipedia"])
            )
            
            # 3. æ±‡æ€»ç»“æœ
            summary = self.summarize_results(query, search_results)
            
            # 4. æ„å»ºæœ€ç»ˆå“åº”
            final_response = {
                "status": "success",
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "analysis": analysis,
                "search_results": search_results,
                "summary": summary,
                "debug_logs": self.debug_info.get_logs()
            }
            
            self.debug_info.add_log("search_complete", {
                "query": query,
                "total_debug_logs": len(self.debug_info.get_logs())
            })
            
            return final_response
            
        except Exception as e:
            error_response = {
                "status": "error",
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
                "debug_logs": self.debug_info.get_logs()
            }
            self.debug_info.add_log("search_error", {"error": str(e)})
            return error_response

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨"""
    # è·å–Google APIå¯†é’¥
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    if not google_api_key:
        print("âš ï¸ æœªæ‰¾åˆ°GOOGLE_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†æ— æ³•ä½¿ç”¨Gemini API")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨.envæ–‡ä»¶ä¸­é…ç½®GOOGLE_API_KEY")
        return
    
    # åˆ›å»ºæ™ºèƒ½æœç´¢Agent
    print("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½æœç´¢Agent...")
    agent = IntelligentSearchAgent(google_api_key)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°ç ”ç©¶è¿›å±•"
    
    print(f"\nğŸ“Œ æµ‹è¯•æŸ¥è¯¢: {test_query}")
    print("="*60)
    
    # æ‰§è¡Œæœç´¢
    result = agent.search(test_query)
    
    # ä¿å­˜å®Œæ•´ç»“æœåˆ°æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"gemini_search_result_{timestamp}.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*60}")
    print("ğŸ¤– Gemini AI æ±‡æ€»ç»“æœ:")
    print('='*60)
    print(result.get("summary", "æ— æ±‡æ€»"))
    
    # æ‰“å°æœç´¢ç»Ÿè®¡
    print(f"\n{'='*60}")
    print("ğŸ“Š æœç´¢ç»Ÿè®¡:")
    print('='*60)
    if "search_results" in result:
        for source, data in result["search_results"].items():
            if data.get("status") == "success":
                count = len(data.get("results", []))
                print(f"âœ… {source}: æ‰¾åˆ° {count} æ¡ç»“æœ")
            else:
                print(f"âŒ {source}: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")

if __name__ == "__main__":
    main() 