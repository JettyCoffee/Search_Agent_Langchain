#!/usr/bin/env python3
"""
arXiv API è®ºæ–‡æŸ¥è¯¢æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯• arXiv API æ˜¯å¦èƒ½æ­£å¸¸è¿æ¥å’Œè·å–è®ºæ–‡æ•°æ®
"""

import sys
import urllib.request
import xml.etree.ElementTree as ET
import time

class ArxivAPITester:
    def __init__(self):
        """åˆå§‹åŒ– arXiv API æµ‹è¯•ç±»"""
        self.base_url = "http://export.arxiv.org/api/query"
        
    def test_basic_query(self):
        """æµ‹è¯•åŸºæœ¬æŸ¥è¯¢åŠŸèƒ½"""
        print("ğŸ” æµ‹è¯•åŸºæœ¬æŸ¥è¯¢åŠŸèƒ½...")
        try:
            # æ„å»ºarXiv APIæŸ¥è¯¢URL
            query = 'search_query=cat:cs.AI+AND+ti:large+language+model&start=0&max_results=5'
            url = f'{self.base_url}?{query}'
            
            # å‘é€è¯·æ±‚
            print(f"   å‘é€è¯·æ±‚åˆ°: {url}")
            with urllib.request.urlopen(url) as response:
                data = response.read().decode('utf-8')
            
            # è§£æXMLå“åº”
            root = ET.fromstring(data)
            
            # æå–å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opensearch': 'http://a9.com/-/spec/opensearch/1.1/',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # è·å–æ€»ç»“æœæ•°
            total_results = root.find('.//opensearch:totalResults', namespaces).text
            print(f"   æ€»ç»“æœæ•°: {total_results}")
            
            # è·å–æ–‡ç« æ¡ç›®
            entries = root.findall('.//atom:entry', namespaces)
            print(f"   è·å–åˆ° {len(entries)} ç¯‡è®ºæ–‡")
            
            # æ‰“å°è®ºæ–‡ä¿¡æ¯
            for i, entry in enumerate(entries, 1):
                title = entry.find('./atom:title', namespaces).text.strip()
                published = entry.find('./atom:published', namespaces).text
                authors = [author.find('./atom:name', namespaces).text for author in entry.findall('./atom:author', namespaces)]
                categories = [category.get('term') for category in entry.findall('./atom:category', namespaces)]
                
                print(f"\n   è®ºæ–‡ {i}:")
                print(f"   æ ‡é¢˜: {title}")
                print(f"   å‘å¸ƒæ—¥æœŸ: {published}")
                print(f"   ä½œè€…: {', '.join(authors[:3])}{'...' if len(authors) > 3 else ''}")
                print(f"   åˆ†ç±»: {', '.join(categories)}")
                
            return True
                
        except Exception as e:
            print(f"âŒ åŸºæœ¬æŸ¥è¯¢æµ‹è¯•å¤±è´¥ï¼š{e}")
            return False
    
    def test_advanced_query(self):
        """æµ‹è¯•é«˜çº§æŸ¥è¯¢åŠŸèƒ½"""
        print("\nğŸ” æµ‹è¯•é«˜çº§æŸ¥è¯¢åŠŸèƒ½...")
        try:
            # æ„å»ºå¤šæ¡ä»¶æŸ¥è¯¢
            query = 'search_query=cat:cs.CV+AND+cat:cs.AI+AND+submittedDate:[20230101+TO+20231231]&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending'
            url = f'{self.base_url}?{query}'
            
            # å‘é€è¯·æ±‚
            print(f"   å‘é€è¯·æ±‚åˆ°: {url}")
            with urllib.request.urlopen(url) as response:
                data = response.read().decode('utf-8')
            
            # è§£æXMLå“åº”
            root = ET.fromstring(data)
            
            # æå–å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opensearch': 'http://a9.com/-/spec/opensearch/1.1/',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # è·å–æ€»ç»“æœæ•°
            total_results = root.find('.//opensearch:totalResults', namespaces).text
            print(f"   æ€»ç»“æœæ•°: {total_results}")
            
            # è·å–æ–‡ç« æ¡ç›®
            entries = root.findall('.//atom:entry', namespaces)
            print(f"   è·å–åˆ° {len(entries)} ç¯‡è®ºæ–‡")
            
            # æ‰“å°è®ºæ–‡ä¿¡æ¯
            for i, entry in enumerate(entries, 1):
                title = entry.find('./atom:title', namespaces).text.strip()
                published = entry.find('./atom:published', namespaces).text
                authors = [author.find('./atom:name', namespaces).text for author in entry.findall('./atom:author', namespaces)]
                
                # è·å–æ‘˜è¦
                summary = entry.find('./atom:summary', namespaces).text.strip()
                summary_short = summary[:100] + "..." if len(summary) > 100 else summary
                
                # è·å–DOI (å¦‚æœæœ‰)
                doi_element = entry.find('./arxiv:doi', namespaces)
                doi = doi_element.text if doi_element is not None else "N/A"
                
                # è·å–PDFé“¾æ¥
                pdf_link = None
                links = entry.findall('./atom:link', namespaces)
                for link in links:
                    if link.get('title') == 'pdf':
                        pdf_link = link.get('href')
                        break
                
                print(f"\n   è®ºæ–‡ {i}:")
                print(f"   æ ‡é¢˜: {title}")
                print(f"   å‘å¸ƒæ—¥æœŸ: {published}")
                print(f"   ä½œè€…: {', '.join(authors[:3])}{'...' if len(authors) > 3 else ''}")
                print(f"   æ‘˜è¦: {summary_short}")
                print(f"   DOI: {doi}")
                print(f"   PDFé“¾æ¥: {pdf_link}")
                
            return True
                
        except Exception as e:
            print(f"âŒ é«˜çº§æŸ¥è¯¢æµ‹è¯•å¤±è´¥ï¼š{e}")
            return False
    
    def test_pagination(self):
        """æµ‹è¯•åˆ†é¡µæŸ¥è¯¢åŠŸèƒ½"""
        print("\nğŸ” æµ‹è¯•åˆ†é¡µæŸ¥è¯¢åŠŸèƒ½...")
        try:
            # å®šä¹‰æŸ¥è¯¢å‚æ•°
            query_base = 'search_query=cat:cs.LG&sortBy=submittedDate&sortOrder=descending'
            results_per_page = 2
            
            # è¿›è¡Œå¤šæ¬¡åˆ†é¡µæŸ¥è¯¢
            for page in range(2):
                start = page * results_per_page
                
                # æ„å»ºå¸¦åˆ†é¡µçš„æŸ¥è¯¢URL
                query = f'{query_base}&start={start}&max_results={results_per_page}'
                url = f'{self.base_url}?{query}'
                
                # å‘é€è¯·æ±‚
                print(f"\n   ç¬¬{page+1}é¡µæŸ¥è¯¢ï¼Œå‘é€è¯·æ±‚åˆ°: {url}")
                with urllib.request.urlopen(url) as response:
                    data = response.read().decode('utf-8')
                
                # è§£æXMLå“åº”
                root = ET.fromstring(data)
                
                # æå–å‘½åç©ºé—´
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'opensearch': 'http://a9.com/-/spec/opensearch/1.1/',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                # è·å–æ€»ç»“æœæ•°å’Œå½“å‰é¡µä¿¡æ¯
                total_results = root.find('.//opensearch:totalResults', namespaces).text
                start_index = root.find('.//opensearch:startIndex', namespaces).text
                items_per_page = root.find('.//opensearch:itemsPerPage', namespaces).text
                
                print(f"   æ€»ç»“æœæ•°: {total_results}")
                print(f"   å½“å‰èµ·å§‹ç´¢å¼•: {start_index}")
                print(f"   æ¯é¡µæ¡ç›®æ•°: {items_per_page}")
                
                # è·å–æ–‡ç« æ¡ç›®
                entries = root.findall('.//atom:entry', namespaces)
                print(f"   æœ¬é¡µè·å–åˆ° {len(entries)} ç¯‡è®ºæ–‡")
                
                # æ‰“å°è®ºæ–‡ä¿¡æ¯
                for i, entry in enumerate(entries, 1):
                    title = entry.find('./atom:title', namespaces).text.strip()
                    published = entry.find('./atom:published', namespaces).text
                    
                    print(f"   è®ºæ–‡ {start+i}:")
                    print(f"   æ ‡é¢˜: {title}")
                    print(f"   å‘å¸ƒæ—¥æœŸ: {published}")
                
                # APIé™åˆ¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if page < 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç­‰å¾…ä¸€ä¸‹
                    print("   ç­‰å¾…3ç§’ä»¥ç¬¦åˆAPIé™åˆ¶...")
                    time.sleep(3)
            
            return True
                
        except Exception as e:
            print(f"âŒ åˆ†é¡µæŸ¥è¯¢æµ‹è¯•å¤±è´¥ï¼š{e}")
            return False
    
    def test_specific_id(self):
        """æµ‹è¯•é€šè¿‡IDæŸ¥è¯¢è®ºæ–‡"""
        print("\nğŸ” æµ‹è¯•é€šè¿‡IDæŸ¥è¯¢è®ºæ–‡...")
        try:
            # ä½¿ç”¨IDç›´æ¥æŸ¥è¯¢è®ºæ–‡
            paper_id = "2305.10403"  # GPT-4è®ºæ–‡ID
            query = f'id_list={paper_id}'
            url = f'{self.base_url}?{query}'
            
            # å‘é€è¯·æ±‚
            print(f"   å‘é€è¯·æ±‚åˆ°: {url}")
            with urllib.request.urlopen(url) as response:
                data = response.read().decode('utf-8')
            
            # è§£æXMLå“åº”
            root = ET.fromstring(data)
            
            # æå–å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'opensearch': 'http://a9.com/-/spec/opensearch/1.1/',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # è·å–è®ºæ–‡æ¡ç›®
            entry = root.find('.//atom:entry', namespaces)
            
            if entry is not None:
                # è·å–è¯¦ç»†ä¿¡æ¯
                title = entry.find('./atom:title', namespaces).text.strip()
                published = entry.find('./atom:published', namespaces).text
                updated = entry.find('./atom:updated', namespaces).text
                summary = entry.find('./atom:summary', namespaces).text.strip()
                authors = [author.find('./atom:name', namespaces).text for author in entry.findall('./atom:author', namespaces)]
                categories = [category.get('term') for category in entry.findall('./atom:category', namespaces)]
                
                # è·å–è¯„è®ºã€DOIç­‰é¢å¤–ä¿¡æ¯
                comment_element = entry.find('./arxiv:comment', namespaces)
                comment = comment_element.text if comment_element is not None else "N/A"
                
                journal_ref_element = entry.find('./arxiv:journal_ref', namespaces)
                journal_ref = journal_ref_element.text if journal_ref_element is not None else "N/A"
                
                doi_element = entry.find('./arxiv:doi', namespaces)
                doi = doi_element.text if doi_element is not None else "N/A"
                
                # æ‰“å°è¯¦ç»†ä¿¡æ¯
                print(f"\n   è®ºæ–‡è¯¦æƒ…:")
                print(f"   ID: {paper_id}")
                print(f"   æ ‡é¢˜: {title}")
                print(f"   å‘å¸ƒæ—¥æœŸ: {published}")
                print(f"   æ›´æ–°æ—¥æœŸ: {updated}")
                print(f"   ä½œè€…: {', '.join(authors)}")
                print(f"   åˆ†ç±»: {', '.join(categories)}")
                print(f"   è¯„è®º: {comment}")
                print(f"   æœŸåˆŠå¼•ç”¨: {journal_ref}")
                print(f"   DOI: {doi}")
                print(f"   æ‘˜è¦æ‘˜å½•: {summary[:150]}...")
                
                return True
            else:
                print(f"âŒ æœªæ‰¾åˆ°IDä¸º {paper_id} çš„è®ºæ–‡")
                return False
                
        except Exception as e:
            print(f"âŒ IDæŸ¥è¯¢æµ‹è¯•å¤±è´¥ï¼š{e}")
            return False
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ arXiv API è®ºæ–‡æŸ¥è¯¢æµ‹è¯•\n")
        print("=" * 50)
        
        tests = [
            ("åŸºæœ¬æŸ¥è¯¢æµ‹è¯•", self.test_basic_query),
            ("é«˜çº§æŸ¥è¯¢æµ‹è¯•", self.test_advanced_query),
            ("åˆ†é¡µæŸ¥è¯¢æµ‹è¯•", self.test_pagination),
            ("IDæŸ¥è¯¢æµ‹è¯•", self.test_specific_id)
        ]
        
        results = []
        for test_name, test_func in tests:
            try:
                result = test_func()
                results.append((test_name, result))
            except Exception as e:
                print(f"âŒ {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")
                results.append((test_name, False))
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 50)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
        
        passed = 0
        for test_name, result in results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"   {test_name}: {status}")
            if result:
                passed += 1
        
        print(f"\næ€»è®¡: {passed}/{len(results)} é¡¹æµ‹è¯•é€šè¿‡")
        
        if passed == len(results):
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼arXiv API è¿æ¥æ­£å¸¸")
            return True
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– API çŠ¶æ€")
            return False


def main():
    """ä¸»å‡½æ•°"""
    tester = ArxivAPITester()
    success = tester.run_all_tests()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
